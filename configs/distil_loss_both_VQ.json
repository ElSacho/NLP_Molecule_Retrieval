{
    "expt_name": "distil_both_VQ",
    "log_dir":"VQ",
    "model_path": "models/oneHead_LinearNLP.py",

    "epochs_decay" : 8,
    "batch_size_add" : 20,
  
    "num_node_features": 300,
    "nout": 300,
    "nhid": 300,
    "graph_hidden_channels": 300,
    "mlp_layers": 2,

    "model_name": "distilbert-base-uncased",
    "num_layers_to_freeze" : 0,
    "VQ": true,
    "num_embeddings": 50000,
    "beta": 0.25,

    "loss" : "triplet and contrastive",
    "t_max" : 0.8,
    "lambda_param" : 0.1,
    "margin_delta" : 0.3,
    "lambda_contra": 1.0,
    "lambda_vq": 0.1,

    "nb_epochs": 200,
    "batch_size": 100,
    "print_every": 50,
  
    "learning_rate": 3e-04,
    "weight_decay": 1.01,
  
    "load_model_path": "None",
    "save_path": "distil_both_loss_VQ.pt"
  }
